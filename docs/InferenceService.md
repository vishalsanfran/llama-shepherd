
## InferenceService

InferenceService defines a logical inference endpoint.

This endpoint:
	â€¢	abstracts away pods, deployments, replicas, services
	â€¢	exposes a consistent model entrypoint
	â€¢	standardizes how inference workloads are handled
	â€¢	becomes a unit of scaling, routing, scheduling, and optimization

This is similar to:

Kubeflow â†’ InferenceService

KServe â†’ ISVC

vLLM â†’ serving endpoint configs

OpenAI â†’ model deployment docs

HuggingFace TGI â†’ model server



What are â€œrouter podsâ€?

A router pod is a Deployment-managed pod that:

ğŸ”¹ Accepts inference requests (HTTP or gRPC)

ğŸ”¹ Queues them

ğŸ”¹ Batches them (future step)

ğŸ”¹ Sends them to worker pods (future LLMModel CRD)

ğŸ”¹ Optionally performs scheduling / concurrency / QoS logic

The operator is the control plane, and router pods are the data plane.